---
title: "Scraping comments from Regulations.Gov"
author: "Harald Klimes, Bennett McIntosh"
date: "2022-07-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(java.parameters = "-Xmx1g") #xlsx export requires several MB of java heap space; 1g is overkill

library(jsonlite)
library(httr)
library(tidyverse)
library(pdftools)
library(dplyr)
library(stringr)
library(xlsx)

api_key <- Sys.getenv("regulations_gov_key")

demo = FALSE #Turn this on to run examples in several of the code blocks below
```

# Background and Prerequisites

For a research project, we need to scrape comments submitted to the Environmental Protection Agency on a proposed rule. The comments are posted on [regulations.gov](https://www.regulations.gov/document/EPA-HQ-OA-2018-0259-9322/comment).

We would like this scraping to produce:

1)  A data frame containing the text of every comment on the docket, as well as assorted metadata about the comments and the docket such as submitter name/type and objectID

2)  A directory containing PDFs (or similar NVivo-legible document) of every comment, in sub-directories named by comment ID for future search/analysis. The documents in this directory should include both the attachments to the comments themselves, as well as a holistic document, with a table of contents, that combines the comment and all attachments

Documentation for the regulations.gov API is available [here](https://open.gsa.gov/api/regulationsgov/). An API key can be requested [here](https://api.data.gov/signup/).

The below code assumes you have saved your API key as "regulations_gov_key" in your `renviron`. We also make use of a few libraries, listed above.

# Affordances and constraints

## Rate Limits

The GSA imposes a [rate limit](https://api.data.gov/docs/rate-limits/) of 1000 requests per hour (on a rolling basis) for the api keys it distributes. To avoid running afoul of these, we use a wrapper for the GET() function that simply pauses execution when we approach the limits. This is not the most efficient solution, since the rate limits function on a rolling basis (so if we made our first request at 10:15 and our 1000th at 10:25 we could start making new requests at 11:15, not 11:25), but it will serve for the scale at which we are working.

NOTE: It seems like the rate limiting doesn't refresh as quickly as we might prefer, so we sleep for an extra 5 minutes

```{r}
get_or_wait <- function(url) {
  
  # GET the item from the URL given.
  # Within a trycatch in case GET() returns an error
  
  r <- NA
  sleepTime <- 60 # if there's a connection issue, try again in a minute
  while (is.na(r[1])) {
  
    r <- tryCatch(
      {
        r <- GET(url)
      }, 
      error=function(cond) {
        message(paste("error in fetching url: ", url))
        message(cond)
        Sys.sleep(sleepTime)
        return(NA)
      }
      
      
    )
  }

  # Your remaining requests before the rate limit are stored in r$headers
  remaining <- as.integer(r$headers$`x-ratelimit-remaining`)
  
  # I got an error once where this was true and have not yet been able to reproduce it, so just in case...
  if (length(remaining < 5) == 0) {
    cat(paste0("\nthat error where remaining < 5 is of length 0 happened again, saving r"))
    saveRDS(r, "badRawComment.Rdata")
  }
  
  # If remaining requests are low, sleep for an hour and then resume execution, reporting the action in "log.txt"
  if (remaining < 5) {
    pauseTime <- lubridate::now()
    cat(paste0("\n APPROACHING RATE LIMIT: pausing until 1hr 5min from ", pauseTime, "\n",
               "requests remaining currently: ", remaining, "\n"),
        file = "log.txt", append = TRUE)
    Sys.sleep(3900)
    restartTime <- lubridate::now()
    r <- GET(url)
    remaining <- as.integer(r$headers$`x-ratelimit-remaining`)
    cat(paste0("Restarting at ", restartTime, "; requests remaining currently: ", remaining, "\n"), 
        file = "log.txt", append = TRUE)
  }
  return(r)
}
```

Note the messages saved to `log.txt`. These (and others below) are the ones I have found most useful in debugging; running this code, especially the download function below, generates many other notes and warnings to `stdout` that I have found less useful, so tend to pipe to pipe to a separate file such as `out.txt`. To facilitate this, both `log.txt` and `out.txt` are currently listed in `.gitignore`

## Page limits

In addition to the rate limits mentioned above, the GSA places a few other limits on the API:

1)  When pulling a page of comments, the page may not be larger than 250 comments

2)  When paging through several pages of comments, you may only access up to 20 pages (i.e. calls for page 21+ of a search are not valid)

This effectively means that no search can return more than 5,000 comments. For dockets with \~9k and \~22k comments, this is an issue! The GSA [currently recommends](https://open.gsa.gov/api/regulationsgov/#there-are-strict-pagination-limits-in-v4-how-do-i-retrieve-all-comments-in-a-docket-posted-on-the-same-day-if-the-number-of-comments-is-greater-than-2500) (in beta versions of v4 of the API) that you get around these issues by sorting your search results by `lastModifiedDate`, and then running several searches in series, each of which filters out items with a `lastModifiedDate` before the most recent result in your previous search. We will need to run 2 different searches for our 9k comments, and 5 for our 22k comments

The function below will assist us in paging through these searches. It gives us a page of comments, sorted by `lastModifiedDate`, from a search in which comments with a `lastModifiedDate` before `last_mod` filtered out if needed. Page size (`pSize`) defaults to the max, 250, but may be set as low as 5 (usually for testing purposes). Note that getting a page of comments on a given document requires that document's objectID, which we will find below.

```{r}
getAPageOfComments <- function(page, objectID, last_mod = "", pSize = 250){
  if (last_mod == "") {
    url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                  objectID, "&page[size]=", pSize, "&page[number]=", page,
                  "&sort=lastModifiedDate,documentId&api_key=", api_key)
  } else {
    # Note that the space in the time string is replaced with '%20'
    url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                  objectID, "&filter[lastModifiedDate][ge]=", sub(" ", "%20", last_mod), 
                  "&page[size]=", pSize, "&page[number]=", page, 
                  "&sort=lastModifiedDate,documentId&api_key=", api_key)
  }
  # print(paste("getting a page of comments from", url))
  raw_comments <- get_or_wait(url)  
  comments <- fromJSON(rawToChar(raw_comments$content)) 
  return(comments)
}
```

# Targeting the right comments

## What are our objectIDs?

Every proposed rule, promulgated rule, comment, or supporting document has a unique `objectID`, a 16-character alphanumeric (since the only letters I've seen are a-f, presumably these are hexidecimal numbers somewhere in GSA's back-end). Within a given rule-making process or "docket," they also share a `docketID`, which is more human-readable. The `docketID`, suffixed by a hyphen and a few numerals, is also listed in each rule, comment, or supporting document as its `id`. So for instance, within the docket with `docketID` "EPA-HQ-OA-2018-0259", we may see a proposed rule with `id` "EPA-HQ-OA-2018-0259-0001", and a comment on said rule with `id` "EPA-HQ-OA-2018-0259-5041", each with a unique hexadecimal `objectID`

We use the `httr` package to retrieve summary information for the docket ID. Here, we're using the docket ID "EPA-HQ-OA-2018-0259" - your docket ID can be discovered by searching for your rule/proposal on [regulations.gov](regulations.gov). Additionally filtering for "Proposed Rule" during the `GET()` call allows us to pull all four documents of `documentType` "Proposed Rule" in one `GET()` call (rather than paging through several pages of documents, mostly of `documentType` "Supporting & Related Material")

```{r}

docketID <- "EPA-HQ-OA-2018-0259"

if (demo) { 
  df <- get_or_wait(url = paste0("https://api.regulations.gov/v4/documents?filter[docketId]=",
                         docketID, "&filter[documentType]=Proposed%20Rule&api_key=",
                         api_key)) 
}
```

With `jsonlite` we can parse the response and identify the object IDs for proposed rule:

```{r}
if (demo) { 
  df2 <- fromJSON(rawToChar(df$content))
  
  df2$data$attributes %>% 
    filter(documentType == "Proposed Rule")
}
```

We can see that the docket includes four documents of documentType "Proposed Rule": the original one from 2018, a modified one in 2020, and one deadline extension for each.

The deadline extensions do not appear to have any associated comments, and have subtype "Extension of Comment Period," which we can use to filter them out.

We can see from df2 that the `objectId` of the 2018 proposed rule is "090000648320bc9e", and for the 2020 supplement is "0900006484450a29". These will allow retrieval of comments through the comments endpoint of the API

For now, we will focus on the 2018 proposed rule:

```{r}
oID <- "090000648320bc9e" 
```

## How many comments?

We will also want to know how many comments we can expect. To do this, we request a page of comments on our proposed rule using its objectID. Note that the URL in this request only returns (in `$data`) the first page of comments meeting the requested criteria (pages default to 25 comments long), but since we are only interested in the `totalElements` field (in `meta`), this does not concern us for now.

```{r}
getCommentCount <- function(objectID){
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                objectID, "&api_key=", api_key)
  
  raw_comments <- get_or_wait(url)
  comments <- fromJSON(rawToChar(raw_comments$content))
  return(comments$meta$totalElements)
}

# get the comment count (will be useful below)
commentCount <- getCommentCount(oID)
if(demo){ commentCount }
```

## Exploring a single comment

When we get a page of comments with an API call like in `getAPageOfComments()` above, we only get limited information about each comment. To get more details, including the content of the comment and its attachments, we need to request the data at each of the urls in `$data$links$self` in turn. For a page of 25 comments, `$data$links$self` will be a list of 25 links, each of which we can pass to the `getCommentByUrl()` function below.

```{r}
getCommentByUrl <- function(link){
  raw_comment <- get_or_wait(paste0(link, "?include=attachments&api_key=", api_key))
  comment <- fromJSON(rawToChar(raw_comment$content))
}

# run the below code to explore the structure of the first comment on the first page
if(demo){
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                oID, "&api_key=", api_key)
  raw_comments <- get_or_wait(url)
  comments <- fromJSON(rawToChar(raw_comments$content))
  single_comment <- getCommentByUrl(comments$data$links$self[1])
  single_comment
}
```

# Downloading attachments

Each comment's details provide link to the attachments for download. Often, there is just a single attachment: the text of the comment; the comment itself will either duplicate this attachment of say something like "see attached." Nevertheless, when we save and return the full text of a comment, we want both the comment body and the content of any attachments. This provides that. All attachments, as well as fullText.txt, are both saved in the directory `attachments/id` where ID is the more human readable ID (the docket ID suffixed with a hyphen and a few numerals). The function also returns the full text, which we will place in a data frame for future analysis.

Most of the body of the getAttachmentDetails function is devoted to handling various rare edge cases:

1)  restricted (often copyrighted) attachments

2)  non-pdf files (which we aren't yet parsing)

3)  files are occasionally available in multiple formats, which may or may not include our chosen format

4)  we may also occasionally encounter items that are not attachments, but I have not encountered this.

While downloading, we're going to extract a few attributes for later analysis - returned as a list localAttributes

1)  \$fullText is the content of everything including all attachments

2)  \$coverAndComment contains the text of the content body, and of "comment.pdf" if it exists - this will catch the content many, but by no means all comments where the body either duplicates or merely directs the reader to an attached comment, while excluding supplementary material

3)  \$attachmentCount should be obvious

4)  \$nonPdfs is the number of attachem

5)  \$restricteds is a boolean that is TRUE if any attachments are restricted (cannot be downloaded at all)

6)  \$authorName attempts to parse the name of the comment's author from the title of the comment, using getAuthorName()

```{r}
getCommentById <- function(id) {
  url <- paste0("https://api.regulations.gov/v4/comments/", id)
  return(getCommentByUrl(url))
}

getAuthorName <- function(comment) {
  title <- comment$data$attributes$title
  if (startsWith(title, "Anonymous")) {return("Anonymous")}
  if (startsWith(title, "Mass Comment Campaign Sponsored")) {
    return(paste(substr(title, 36, 1000), "(mass mail)"))
  }
  if (startsWith(title, "Mass Comment Campaign sponsored")) {
    return(paste(substr(title, 36, 1000), "(mass mail)"))
  }
  if (startsWith(title, "Mass Comment Campaign sponsoring organization unknown")) {
    return("unknown organization (mass mail)")
  }
  if (startsWith(title, "Comment submitted by")) {
    return (substr(title, 22, 1000))
  }
  
  # if we haven't met the previous patterns, give up
  return(title)
}

getAttachmentDetails <- function(comment, destdir = "attachments"){

  # create the directory/s in which to download attachments
  if (!dir.exists(destdir)) { dir.create(destdir)}
  
  commentDir <- comment$data$id
  if (!dir.exists(paste(destdir, commentDir, sep = "/"))) {
    dir.create(paste(destdir, commentDir, sep = "/"))
  }
  
  localAttributes <- list(fullText = "", coverAndComment = "", attachmentCount = 0, 
                          nonPdfs = 0, restricteds = 0, authorName = "")
  
  localAttributes$authorName <- getAuthorName(comment)
  
  # add the text from the comment itself to the fullText string and the coverAndComment string
  localAttributes$fullText <- comment$data$attributes$comment
  localAttributes$coverAndComment <- comment$data$attributes$comment
  
  # if there are attachments, add them to the directory and to the fullText string
  if (!is.null(comment$included)) {

  
    # check the number of attachments
    N <- length(comment$included$id)
    localAttributes$attachmentCount <- N
    
    # if there's a single, restricted attachment, $fileFormats is NA, so in-loop test won't work
    # restricted <- is.na(comment$included$attributes$fileFormats)
    
    for (i in 1:N){
      # to download files in order, you'd think you could iterate by j and comment in the below
      # i <- comment$included$attributes$docOrder[j]
      # print(paste("item", j, "is, on regs.gov, #", i))
      # but occasionally docOrder is not only unordered but non-contiguous (eg 2, 4, 1), so this breaks some stuff
      
      # tests if the file is restricted. 
      # Unfortunately this works differently for single-attachment and multi-attachment comments
      if (is.na(comment$included$attributes$fileFormats)[1] | # 1 attchmt
          is.null(comment$included$attributes$fileFormats[[i]])) { # >1 attchmt
        cat(paste("WARNING: attachment", i, "could not be downloaded from comment ",
                  comment$data$id, ", perhaps because of restrictReasonType: ", 
                  comment$included$attributes$restrictReasonType[i], "\n"),
            file="log.txt", append =TRUE)
        localAttributes$fullText <- paste(localAttributes$fullText, "\n\n---\n", "Attachment", i, 
                                          "title:\n", comment$included$attributes$title[i], 
                                          "\n---\n", "RESTRICTED (",comment$included$attributes$restrictReasonType[i], ")",
                                          sep = " ")
        localAttributes$restricteds <- localAttributes$restricteds + 1
        next
      }
      
      itemFormat <- comment[["included"]][["attributes"]][["fileFormats"]][[i]][[2]]
      
      # if the item can be downloaded in several formats, set to PDF if possible
      if (length(itemFormat) > 1) {
        if ("pdf" %in% itemFormat) {
          fi <- match("pdf", itemFormat)
          itemFormat <- "pdf"
        } else {
          cat(paste("multiple formats, but none PDF; comment:", comment$data$id, "attachment: ", i, "\n"), 
              file="log.txt", append =TRUE)
          fi <- 1
        }
        itemFormat <- comment[["included"]][["attributes"]][["fileFormats"]][[i]][[2]][[fi]]
      } else {fi <- 1}
      
      
      
      # Warn the user if the file is not marked as an attachment or is not a pdf
      if (comment$included$type[i] != "attachments") {
        cat(paste0("WARNING, item ", i, "'s type in comment ", comment$data$id,
                   " is not 'attachments' but", comment$included$type, "\n"), 
            file="log.txt", append =TRUE)
        localAttributes$nonPdfs <- localAttributes$nonPdfs + 1
      }
      
      # target the directory, filename, and extension
      # to avoid path errors, slashes ("/") in the title are replaced with hyphens ("-")
      j <- comment$included$attributes$docOrder[i]
      title <- paste0(sprintf("a%02d ", j),
                      gsub("/", "-", comment$included$attributes$title[i]))
      if (nchar(title) > 64) {title <- substr(title, 1, 64)}
      filename <- paste0(title, ".", itemFormat)
      destpath <- paste(destdir, commentDir, filename, sep = "/")
      
      # print(paste0("found ", filename, " an ", comment$included$type[i], " with format ",
      # itemFormat))
      
      # download the file to the targeted path
      download.file(comment$included$attributes$fileFormats[[i]]$fileUrl[fi], 
                    destfile = destpath, 
                    mode = "wb")
      
      # Warn the user if the file is not a PDF (and add placeholder to fullText)
      if (itemFormat != "pdf") {
        cat(paste0("WARNING, item ", i, "'s format in comment ", comment$data$id,
                   " is not 'pdf' but ", itemFormat, "\n"), file="log.txt", append =TRUE)
        localAttributes$fullText <- paste(localAttributes$fullText, "\n\n---\n", "Attachment", i, 
                                          "title:\n", comment$included$attributes$title[i],
                                          "\n---\n\n", "UNPARSED ", itemFormat, sep = " ")
      } # Otherwise, download the PDF and grab the text 
      else {
        attachmentText <- paste(pdf_text(destpath), collapse = "", sep = "/n")
        
        localAttributes$fullText <- paste(localAttributes$fullText, "\n\n---\n", "Attachment", j, "title:\n",
                          comment$included$attributes$title[i], "\n---\n\n", 
                          attachmentText, sep = " ")
        if (title == "Comment") {
          localAttributes$coverAndComment <- paste(localAttributes$coverAndComment, "\n\n---- Comment.pdf ----\n\n",
                                                   attachmentText)
        }
        
      }
      
    }
    # TODO: combine all downloaded files into one big file - but idk if that's needed rn
    # TODO: consider simply write the new text to a file progressively (r/t storing as a string)
    #       (if so fullText can be a path if you want to do searching later)
    # TODO: fix so docs are downloaded/appended to .txt in order by the $docOrder list in $included$attributes
  }

  textFName <- paste(destdir, commentDir, "fullText.txt", sep = "/")
  cat(localAttributes$fullText, file = textFName, append = FALSE)

  
  return(localAttributes)
} 

testDownload <- function(id) {
  comment <- getCommentById(id)
  getAttachmentDetails(comment)
  return(comment)
} 


if (demo) {
  c1037 <- testDownload("EPA-HQ-OA-2018-0259-1037") # multiple attachments, 1 restricted
  c0066 <- testDownload("EPA-HQ-OAR-2020-0532-0066") # multiple attachments, none restricted; #13 needs OCR
  c0445 <- testDownload("EPA-HQ-OA-2018-0259-0445") # single, restricted (copyrighted) attachment
  c0684 <- testDownload("EPA-HQ-OA-2018-0259-0684") # single, restricted (other) attachment
  c0671 <- testDownload("EPA-HQ-OA-2018-0259-0671") # attachment is docx, not pdf
  c1388 <- testDownload("EPA-HQ-OA-2018-0259-1388") # no attachment (comment body only)
  c9240 <- testDownload("EPA-HQ-OA-2018-0259-9240") # slashes in filename
  c8272 <- testDownload("EPA-HQ-OA-2018-0259-8272") # 14 attchmts but document order skips 12 and reaches 15
  c9299 <- testDownload("EPA-HQ-OA-2018-0259-9299") # very long document title
}


```

The `if` statement above pulls some comments I have found useful for testing various edge cases

# Putting it together

## Helper functions

Before we finally run the request, a few helper functions help us log which comments have been downloaded and put all the comments together into a data frame.

```{r}
# nullToNA replaces NULL elements of a list with NA
nullToNA <- function(x) {
  x[sapply(x, is.null)] <- NA
  return(x)
}

# slug takes a string with at least one hyphen and extracts everything after the last hyphen
# so for a comment's ID EPA-HQ-OA-2018-0259-####(#), it would extract the suffixed numbers
# other agencies may have other ID formats, but "everything after last hyphen" seems like a decent slug for most dockets
slug <- function(id) {
  numSegments <- str_count(id, "-") + 1
  return(word(id, numSegments, sep = "-"))
}

# fixTimeString takes the time in the form yyyy-mm-ddThh:mm:ssZ (as the API gives it) 
# and returns yyyy-mm-dd hh:mm:ss (in Eastern Time) (as the API demands). 
# Note that in URLs the space must be replaced with "%20"; eg in getPageOfComments() 
fixTimeString <- function(yyyymmddThhmmssZ) {
  last_mod_UTC <- as.POSIXct(sub("T", " ", yyyymmddThhmmssZ), tz = "UTC")
  last_mod <- sub(" E*T", "", lubridate::with_tz(last_mod_UTC,
                                                 "America/New_York"))
  return(last_mod)
}
```

## Page by page and batch by batch

The below uses nested loops in the form:

`while (not enough comments) {`

`for (each page in a batch) {`

`for (each comment on a page) {do stuff}}}}`

to add comments one by one to a data frame that carries their `attributes`, both those pulled from the comment itself and from the getAttachmentDetails() function.

```{r, echo = FALSE}

# getCommentDf <- function(oID) {

  startTime <- lubridate::now()

  pageLen <- 250 
  pagesPerBatch <- 20 
  commentCount <- getCommentCount(oID)
  
  # initialize some tracking variables
  last_mod <- ""
  batchN <- 1  
  commentsAdded <- 0
  
  attachmentsDest <- "tempattchmts"

  
    # if previous download was interrupted partway through, can specify starting with a particular comment
  # otherwise, this should be set to ""
  # currently there is no way to save the dataframe from interrupted downloads, so only use this for testing and fetching files, not comment data
  startCommentID <- ""
  
  # if you know how many comments deep the interrupt was, you can set that here so the script ends smoothly
  if (startCommentID != "") {
    startComment <- getCommentById(startCommentID)
    last_mod <- fixTimeString(startComment$data$attributes$modifyDate)
    commentsAdded <- 7560 
  }

  # you may also set the comment count lower for testing purposes if you want
  # commentCount <- 900
  # textList <- rep("ID TBD", commentCount)
  
  commentDF <- as.data.frame(list(no = "data"))
  
  cat(paste(startTime, ": Pulling", commentCount, "comments in batches of", pagesPerBatch, "pages of",
            pageLen, "comments each"), file = "log.txt", append = FALSE)
  
  stop = FALSE
  while (!stop) {
    
    for (pageN in 1:pagesPerBatch) {
      
      # don't bother adding more comments if we have all the comments we need
      # this will usually be the stop condition when collecting a full set of comments
      if(commentsAdded == commentCount) { 
        stop = TRUE
        break 
      }
      
      # pull a page of comments
      cat(paste("\npulling page", pageN, "of batch", batchN, "\n"), 
          file="log.txt", append =TRUE)
      pageOfComments <- getAPageOfComments(pageN, oID, last_mod, pageLen)
      cat(paste(" with", pageOfComments$meta$numberOfElements, "elements\n"),
          file="log.txt", append =TRUE)
      cat(paste(" adding these to the", commentsAdded, "elements already added\n"),
           file="log.txt", append =TRUE)
      
      # for the first page on every batch but the first, find the comment where we left off
      if ((pageN == 1) & (commentsAdded != 0)) { 

        start <- match(last_name, pageOfComments$data$id) + 1
        cat(paste0("starting batch ", batchN," at ", start, "th comment\n"), file="log.txt", append =TRUE)
      } else {
        start <- 1
      }
      
      
      # get every comment in the page, except the ones before the last comment we grabbed
      for (i in start:pageLen) {
        
        commentsAdded <- commentsAdded + 1
        
        # stop comment collection early if we've explicitly asked for fewer comments than exist
        if (commentsAdded > commentCount) {
          cat(paste0("WARNING: it looks like there's more comments than commentCount (",
                     commentCount, ")! Terminating..."), file = "log.txt", append = TRUE)
          stop = TRUE
          break
        }
        
        url <- paste0(pageOfComments$data$links$self[i])
        comment <- getCommentByUrl(url)
        
        # log that the comment has been pulled
        cat(paste0(commentsAdded, ":-", slug(comment$data$id), "..."), file="log.txt", append = TRUE)
        
        # put the comment's details into a data frame
        comment$data$attributes["displayProperties"] <- NA
        attsList <- comment$data$attributes[c('commentOn', 'commentOnDocumentId', 'duplicateComments', 
                                              'comment', 'docketId', 'documentType', 'objectId', 
                                              'modifyDate', 'pageCount', 'postedDate', 'postmarkDate', 
                                              'receiveDate', 'subtype', 'title', 'withdrawn')]
        localAtts <- getAttachmentDetails(comment, destdir = attachmentsDest)
        attsList <- nullToNA(c(list(ID = comment$data$id), localAtts, attsList))
        if (commentsAdded == 1) {
          commentDF <- as.data.frame(attsList)
        } else { 
          commentDF <- rbind.data.frame(commentDF, as.data.frame(attsList))
        }

        # names(textList)[commentsAdded] <- comment$data$id
        # textList[commentsAdded] <- getAttachmentDetails(comment)
        # 
        #delete directory to save drive space during testing
        # deldir <- paste("tempattachments", comment$data$id, sep = "/")
        # unlink(deldir, recursive = TRUE)
      
      }
  
    }
    # The last_mod given is formatted yyyy-mm-ddThh:mm:ssZ (in UTC) but get() needs yyyy-mm-dd hh:mm:ss (in Eastern Time) 
    last_mod <- fixTimeString(pageOfComments$data$
                                attributes$lastModifiedDate[pageLen])
    last_name <- comment$data$id
    batchN <- batchN + 1
  }
  
  # zip the comments' attributes together with their names and text (deprecated)
  # c <- cbind(names(textList), textList, commentDF)
  # colnames(c)[1] <- "ID"
  # colnames(c)[2] <- "full text"
  # 
  save(commentDF, file = "text9k.Rdata")
  # 
  # cSmol <- subset(c, select = c(ID, `full text`, commentOnDocumentId, duplicateComments, 
  #                               comment, docketId, documentType, objectId, modifyDate, pageCount, 
  #                               postedDate, postmarkDate, receiveDate, subtype, title))
  # 
  wb <- createWorkbook()
  s <- createSheet(wb, sheetName = "comments")
  
  addDataFrame(commentDF, s, col.names = TRUE, row.names = FALSE, byrow = FALSE)
  saveWorkbook(wb, "text9k_try2.xlsx")
  
  # log the total time and time per comment
  endTime <- lubridate::now()
  runTime <- endTime - startTime
  timePerComment <- runTime / commentCount
  cat(paste("Run time of", runTime, "or", timePerComment, "per comment"))

  
```

# Next Steps

3)  Test rate limit handling and run through while deleting files to find edge cases
4)  OCR with PDFtools?
5)  Make unified PDF 

5.1) including of multi-part comments like EDF (9240 &c) and NRDC (9299 &c) 

5.2) fix document ordering (removed because sometimes seems discontinuous?)

6)  Statistical characterization:

<!-- -->

a)  How many non-unique comments
b)  Distribution of n_attachments

-   Number of attachments?
-   Is there a comment.pdf attachment?
-   Are there non-pdf attachments?
-   Are there restricted attachments?
-   body + comment.pdf
-   grab author name?

c)  Distribution of time submitted
d)  Any of these interact with institutional/individual/what was the other one categories?
e)  How many documents need OCR-ing?

<!-- -->

4)  How many documents are restricted or otherwise need manual attention?

<!-- -->

7)  Sort functions
8)  Search functions
9)  As a QOL change for future users, might be easier if we're sorting by comment ID or object ID than modify date as GSA proposes
