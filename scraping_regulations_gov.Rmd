---
title: "Scraping comments from Regulations.Gov"
author: "Harald Klimes, Bennett McIntosh"
date: "2022-07-21"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# .xlsx export requires several MB of java heap space; so 1g is prudent overkill
options(java.parameters = "-Xmx1g") 

# Load helper functions -- these are the functions mentioned in the text below
source("functions/regulations_gov_functions.R")

# Load the api_key from environmental variables
api_key <- Sys.getenv("regulations_gov_key")

# Set demo = TRUE to run examples in several of the code blocks below
demo = FALSE 
```

# Background and Prerequisites

For a research project, we need to scrape comments submitted to the Environmental Protection Agency on a proposed rule. The comments are posted on [regulations.gov](https://www.regulations.gov/document/EPA-HQ-OA-2018-0259-9322/comment).

We would like this scraping to produce:

1)  A data.frame containing the text of every comment on the docket, as well as assorted metadata about the comments and the docket such as submitter name/type and objectID

2)  A List of the comments in the docket, in the form they are downloaded

3)  A directory containing PDFs (or similar NVivo-legible document) of every comment, in sub-directories named by comment ID for future search/analysis. The documents in this directory should include both the attachments to the comments themselves, as well as a holistic document, with a table of contents, that combines the comment and all attachments

Documentation for the regulations.gov API is available [here](https://open.gsa.gov/api/regulationsgov/). An API key can be requested [here](https://api.data.gov/signup/).

The below code assumes you have saved your API key as "regulations_gov_key" in your `renviron`. We also make use of a few libraries, listed above.

# Affordances and constraints

## Rate Limits

The GSA imposes a [rate limit](https://api.data.gov/docs/rate-limits/) of 1000 requests per hour (on a rolling basis) for the api keys it distributes. To avoid running afoul of these, we use a wrapper for the GET() function that simply pauses execution when we approach the limits. This is not the most efficient solution, since the rate limits function on a rolling basis (so if we made our first request at 10:15 and our 1000th at 10:25 we could start making new requests at 11:15, not 11:25), but it will serve for the scale at which we are working.

Unfortunately, it seems like the rate limiting doesn't refresh as quickly as we might prefer, so we sleep for an extra 5 minutes (1h5m). The GSA does have a way of requesting an increase to your rate limit. Since we will mostly be working with files offline, we haven't done that, but you may want to if you're grabbing many thousands of comments.

```{r, message=FALSE}
if (demo) { print(get_or_wait) }
```

Note the messages in the above. These (and others below) are the ones I have found most useful in debugging; running this code, especially the download function below, generates many other messages to `stdout` that I have found less useful and try to silence.

## Helper functions

Before we run the request, a few helper functions help us log which comments have been downloaded and put all the comments together into a data frame.

-   `nullToNA()` replaces NULL elements of a list with NA
-   `slug()` takes a string & returns the characters after the last instance of sep
    -   so it can be used to return the file extension (sep = "\\\\.")
    -   or the last digits of an ID (sep = "-"; EPA-HQ-OA-2018-0259-#### -\> \####)
-   Times are stored in regulations.gov data in a different form than the API expects when you're making requests; `fixTimeString()` takes the time in the form `"yyyy-mm-ddThh:mm:ssZ"` (as given in comment data) and returns `"yyyy-mm-dd hh:mm:ss"` (in Eastern Time) (as the API demands).
    -   Note that in request URLs the space in the must be replaced with `"`%20"\`; *eg.* in getPageOfComments()

```{r, message=FALSE}

if (demo) { print(nullToNA) }
if (demo) { print(slug) }
if (demo) { print(fixTimeString) }

```

## Page limits

In addition to the rate limits mentioned above, the GSA places a few other limits on the API:

1)  When pulling a page of comments, the page may not be larger than 250 comments

2)  When paging through several pages of comments, you may only access up to 20 pages (i.e. calls for page 21+ of a search are not valid)

This effectively means that no search can return more than 5,000 comments. For dockets with \~9k and \~22k comments, this is an issue! The GSA [currently recommends](https://open.gsa.gov/api/regulationsgov/#there-are-strict-pagination-limits-in-v4-how-do-i-retrieve-all-comments-in-a-docket-posted-on-the-same-day-if-the-number-of-comments-is-greater-than-2500) (in beta versions of v4 of the API) that you get around these issues by sorting your search results by `lastModifiedDate`, and then running several searches in series, each of which filters out items with a `lastModifiedDate` before the most recent result in your previous search. We will need to run 2 different searches for our 9k comments, and 5 for our 22k comments

We use `getAPageOfComments` to scroll through those searches. It gives us a page of comments, sorted by `lastModifiedDate`, from a search in which comments with a `lastModifiedDate` before `last_mod` can be filtered out. Page size (`pSize`) defaults to the max, 250, but may be set as low as 5 (usually for testing purposes). Note that getting a page of comments on a given document requires that document's objectID, which we will find below.

```{r, message=FALSE}
if (demo) { print(getAPageOfComments) }
```

# Targeting the right comments

## What are our objectIDs?

Every proposed rule, promulgated rule, comment, or supporting document has a unique `objectID`, a 16-character alphanumeric (since the only letters I've seen are a-f, presumably these are hexidecimal numbers somewhere in GSA's back-end). Within a given rule-making process or "docket," they also share a `docketID`, which is more human-readable. The `docketID`, suffixed by a hyphen and a few numerals, is also listed in each rule, comment, or supporting document as its `id`. So for instance, within the docket with `docketID` "EPA-HQ-OA-2018-0259", we may see a proposed rule with `id` "EPA-HQ-OA-2018-0259-0001", and a comment on said rule with `id` "EPA-HQ-OA-2018-0259-5041", each with a unique hexadecimal `objectID`

We use the `httr` package to retrieve summary information for the docket ID. Here, we're using the docket ID "EPA-HQ-OA-2018-0259" - your docket ID can be discovered by searching for your rule/proposal on [regulations.gov](regulations.gov). Additionally filtering for "Proposed Rule" during the `GET()` call allows us to pull all four documents of `documentType` "Proposed Rule" in one `GET()` call (rather than paging through several pages of documents, mostly of `documentType` "Supporting & Related Material")

```{r, message=FALSE}

docketID <- "EPA-HQ-OA-2018-0259"

if (demo) { 
  df <- get_or_wait(url = paste0("https://api.regulations.gov/v4/documents?filter[docketId]=",
                         docketID, "&filter[documentType]=Proposed%20Rule&api_key=",
                         api_key)) 
}
```

With `jsonlite` we can parse the response and identify the object IDs for proposed rule:

```{r, message=FALSE}
if (demo) { 
  df2 <- fromJSON(rawToChar(df$content))
  
  df2$data$attributes %>% 
    filter(documentType == "Proposed Rule")
}
```

We can see that the docket includes four documents of documentType "Proposed Rule": the original one from 2018, a modified one in 2020, and one deadline extension for each.

The deadline extensions do not have any associated comments, and have subtype "Extension of Comment Period," which we could use to filter them out if needed, but here we'll just get the `objectID`s manually. We can see from df2 that the `objectId` of the 2018 proposed rule is "090000648320bc9e", and for the 2020 supplement is "0900006484450a29". These will allow retrieval of comments through the comments endpoint of the API

For now, we will focus on the 2018 proposed rule:

```{r, message=FALSE}
oID <- "0900006484450a29" 
```

## How many comments?

We will also want to know how many comments we can expect. To do this, we request a page of comments on our proposed rule using its objectID. Note that the URL in this request only returns (in `$data`) the first page of comments meeting the requested criteria (pages default to 25 comments long), but since we are only interested in the `totalElements` field (in `meta`), this does not concern us for now.

```{r, message=FALSE}

# get the comment count (will be useful below)
commentCount <- getCommentCount(oID)
if(demo){ 
  print(getCommentCount) 
  print("commentCount = ", commentCount)
  }
```

## Exploring a single comment

When we get a page of comments with an API call like in `getAPageOfComments()` above, we only get limited information about each comment. To get more details, including the content of the comment and its attachments, we need to request the data at each of the comments' urls (in `$data$links$self`) in turn. For a page of 25 comments, `$data$links$self` will be a list of 25 links, each of which we can pass to the `getCommentByUrl()` function.

```{r, message=FALSE}

# run the below code to explore the structure of the 1st comment on the 1st page
if(demo){
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                oID, "&api_key=", api_key)
  raw_comments <- get_or_wait(url)
  comments <- fromJSON(rawToChar(raw_comments$content))
  single_comment <- getCommentByUrl(comments$data$links$self[1])
  single_comment
}
```

# Downloading attachments

Each comment's details provide links to the attachments for download. Often, there is just a single attachment: the text of the comment; the comment itself will either duplicate this attachment of say something like "see attached." Nevertheless, when we save and return the full text of a comment, we want both the comment body and the content of any attachments. The function `getAttachmentDetails()` provides that, along with some other characterization.

This function takes a comment (in the list-of-lists form provided by the API), and returns a list `localAttributes` of the data listed below. It also takes a directory name `destdir` (default `"attachments"`). If the boolean argument `doDownload` is `TRUE` (the default), it downloads the attachments a subdirectory of `destdir` named after the comment's ID (*eg.* attachments/EPA-HQ-OA-2018-0259-1234); otherwise, it assumes you have already downloaded all attachments (or at least the non-restricted PDFs) to that subdirectory. 

1)  `$fullText` is the text of everything including the comment body all attachments. At this point all restricted files and all non-PDF files are skipped. 

2)  `$coverAndComment` contains the text of the comment body, and of "Comment.pdf" if it exists

    -   this will catch the content of many, but by no means all comments where the body either duplicates or merely directs the reader to an attached comment, while excluding supplementary material

3)  `$attachmentCount` is the total number of attachments (whether or not they could be downloaded/parsed)

4)  `$nonPdfs` is the number of attachments that are not PDFs (*ie.* that pdftools cannot parse)

5)  `$restricteds` is the number of restricted documents (which cannot be downloaded)

6)  `$authorName` attempts to parse the name of the comment's author from the title of the comment, using `getAuthorName()`. If it can't, the function simply uses the entire title. This works on comments for the EPA proposed rules I've seen, but for other sorts of comments it may produce a field that mostly just replicates the "title" field

Most of the body of the `getAttachmentDetails()` function and the functions it calls are devoted to handling various rare but significant edge cases:

-  restricted (often copyrighted) attachments

-  non-pdf files (which we aren't yet parsing)

-  files are occasionally available in multiple formats, which may or may not include our chosen format

-  we may also occasionally encounter items that are not attachments, but I have not encountered this.

**For more on these and other edge cases in the data, see the README file (once I've added that info)**

```{r, message=FALSE}

testDownload <- function(id) {
  comment <- getCommentById(id)
  localAtts <- getAttachmentDetails(comment)
  return(c(localAtts, comment))
} 

if (demo) {
  c1037 <- testDownload("EPA-HQ-OA-2018-0259-1037") # multiple attachments, 1 restricted
  c0066 <- testDownload("EPA-HQ-OAR-2020-0532-0066") # multiple attachments, none restricted; #13 needs OCR
  c0445 <- testDownload("EPA-HQ-OA-2018-0259-0445") # single, restricted (copyrighted) attachment
  c0684 <- testDownload("EPA-HQ-OA-2018-0259-0684") # single, restricted (other) attachment
  c0671 <- testDownload("EPA-HQ-OA-2018-0259-0671") # attachment is docx, not pdf
  c1388 <- testDownload("EPA-HQ-OA-2018-0259-1388") # no attachment (comment body only)
  c9240 <- testDownload("EPA-HQ-OA-2018-0259-9240") # slashes in filename
  c8272 <- testDownload("EPA-HQ-OA-2018-0259-8272") # 14 attchmts but document order skips 12 and reaches 15
  c9299 <- testDownload("EPA-HQ-OA-2018-0259-9299") # very long document title
}


```

The `if` statement above pulls some comments I have found useful for testing various edge cases

# Putting it together

## Page by page and batch by batch

The below uses nested loops in the form:

`while (not enough comments) {`

`for (each page in a batch) {`

`for (each comment on a page) {do stuff}}}}`

to add comments one by one to a data frame that carries their `attributes`, both those pulled from the comment itself and from the getAttachmentDetails() function.

```{r, echo = FALSE, message=FALSE}

# getCommentDf <- function(oID) {

  startTime <- lubridate::now()

  pageLen <- 250 
  pagesPerBatch <- 20 
  commentCount <- getCommentCount(oID)
  
  # initialize some tracking variables
  last_mod <- ""
  batchN <- 1  
  commentsAdded <- 0
  
  attachmentsDest <- "/Users/Bennett/Library/CloudStorage/Box-Box/EPA_Transparency_Comments/Attachments/proposal_comments_2020" # For syncing with Box, use 
  #/Users/[yourname]/Library/CloudStorage/Box-Box/EPA_Transparency_Comments/whatever on mac equivalent directory on your device

  
  # if previous download was interrupted partway through, can specify starting with a particular comment
  # otherwise, this should be set to ""
  # currently there is no way to save the dataframe from interrupted downloads, so only use this for testing and fetching files, not comment data
  startCommentID <- ""
  
  # if you know how many comments deep the interrupt was, you can set that here so the script ends smoothly
  if (startCommentID != "") {
    startComment <- getCommentById(startCommentID)
    last_mod <- fixTimeString(startComment$data$attributes$modifyDate)
    commentsAdded <- 7560 
  }

  # you may also set the comment count lower for testing purposes if you want
  # commentCount <- 20
  # textList <- rep("ID TBD", commentCount)
  
  commentDF <- as.data.frame(list(no = "data"))
  commentList <- vector(mode = "list", length = commentCount)
  
  message(paste(startTime, ": Pulling", commentCount, "comments in batches of", pagesPerBatch, "pages of",
            pageLen, "comments each"))
  
  stop = FALSE
  while (!stop) {
    
    for (pageN in 1:pagesPerBatch) {
      
      # don't bother adding more comments if we have all the comments we need
      # this will usually be the stop condition when collecting a full set of comments
      if(commentsAdded == commentCount) { 
        stop = TRUE
        break 
      }
      
      # pull a page of comments
      message(paste("\npulling page", pageN, "of batch", batchN, "\n"))
      pageOfComments <- getAPageOfComments(pageN, oID, last_mod, pageLen)
      thisPageLen <- pageOfComments$meta$numberOfElements
      message(paste(" with", thisPageLen, "elements\n"))
      message(paste(" adding these to the", commentsAdded, "elements already added\n"))
      
      # for the first page on every batch but the first, find the comment where we left off
      if ((pageN == 1) & (commentsAdded != 0)) { 
        start <- match(last_name, pageOfComments$data$id) + 1
        message(paste0("starting batch ", batchN," at ", start, "th comment\n"))
      } else {
        start <- 1
      }
      
      # get every comment in the page, except the ones before the last comment we grabbed
      for (i in start:thisPageLen) {
        
        commentsAdded <- commentsAdded + 1
        
        # stop comment collection early if we've explicitly asked for fewer comments than exist
        if (commentsAdded > commentCount) {
          message(paste0("WARNING: it looks like there's more comments than commentCount (",
                     commentCount, ")! Terminating...\n"))
          stop = TRUE
          break
        }
        
        url <- paste0(pageOfComments$data$links$self[i])
        comment <- getCommentByUrl(url)
        
        # log that the comment has been pulled
        message(paste0(commentsAdded, ":-", slug(comment$data$id), "..."))
        
        # put the comment's details into a data frame
        comment$data$attributes["displayProperties"] <- NA
        attsList <- comment$data$attributes[c('commentOn', 'commentOnDocumentId', 'duplicateComments', 
                                              'comment', 'docketId', 'documentType', 'objectId', 
                                              'modifyDate', 'pageCount', 'postedDate', 'postmarkDate', 
                                              'receiveDate', 'subtype', 'title', 'withdrawn')]
        localAtts <- getAttachmentDetails(comment, destdir = attachmentsDest)
        attsList <- nullToNA(c(list(ID = comment$data$id), localAtts, attsList))
        comment$data$attributes <- attsList
        
        # Save the comment and its attributes to both a list of comments and to a dataframe for later manipulation
        commentList[commentsAdded] <- comment
        if (commentsAdded == 1) {
          commentDF <- as.data.frame(attsList)
        } else { 
          commentDF <- rbind.data.frame(commentDF, as.data.frame(attsList))
        }

        # names(textList)[commentsAdded] <- comment$data$id
        # textList[commentsAdded] <- getAttachmentDetails(comment)
        # 
        #delete directory to save drive space during testing
        # deldir <- paste("tempattachments", comment$data$id, sep = "/")
        # unlink(deldir, recursive = TRUE)
      
      }
  
    }
    # The last_mod given is formatted yyyy-mm-ddThh:mm:ssZ (in UTC) but get() needs yyyy-mm-dd hh:mm:ss (in Eastern Time) 
    last_mod <- fixTimeString(pageOfComments$data$
                                attributes$lastModifiedDate[pageLen])
    last_name <- comment$data$id
    batchN <- batchN + 1
  }
  
  # zip the comments' attributes together with their names and text (deprecated)
  # c <- cbind(names(textList), textList, commentDF)
  # colnames(c)[1] <- "ID"
  # colnames(c)[2] <- "full text"
  # 
  save(commentDF, file = "/Users/Bennett/Library/CloudStorage/Box-Box/EPA_Transparency_Comments/Data/08_07_commentDf20.RData")
  save(commentList, file = "/Users/Bennett/Library/CloudStorage/Box-Box/EPA_Transparency_Comments/Data/08_07_commentList20.RData")
  # 
  # cSmol <- subset(c, select = c(ID, `full text`, commentOnDocumentId, duplicateComments, 
  #                               comment, docketId, documentType, objectId, modifyDate, pageCount, 
  #                               postedDate, postmarkDate, receiveDate, subtype, title))
  # 
  wb <- createWorkbook()
  s <- createSheet(wb, sheetName = "comments")
  
  addDataFrame(commentDF, s, col.names = TRUE, row.names = FALSE, byrow = FALSE)
  saveWorkbook(wb, "/Users/Bennett/Library/CloudStorage/Box-Box/EPA_Transparency_Comments/Data/08_07_commentTable20.xlsx")
  
  # log the total time and time per comment
  endTime <- lubridate::now()
  runTime <- endTime - startTime
  timePerComment <- runTime / commentCount
  message(paste("Run time of", runTime, "or", timePerComment, "per comment"))

  
```

# Possible Improvements

2)  Make unified PDF of all downloaded attachments

2.1) including combining multi-part comments like EDF (9240 &c) and NRDC (9299 &c)

4)  OCR with PDFtools?

5)  Statistical characterization:

<!-- -->

a)  Types of comments
b)  Distribution of n_attachments

-   Number of attachments?
-   Is there a comment.pdf attachment?
-   Are there non-pdf attachments?
-   Are there restricted attachments?
-   body + comment.pdf
-   grab author name?

c)  Distribution of time submitted
d)  Any of these interact with institutional/individual/what was the other one categories?
e)  How many documents need OCR-ing?

<!-- -->

4)  How many documents are restricted or otherwise need manual attention?

<!-- -->

7)  Sort functions
8)  Search functions
9)  As a QOL change for future users, might be easier if we're sorting by comment ID or object ID than modify date as GSA proposes

\*\* 10) Look for homology with mass mail comments -- code for text matching For mad-libs -- pull distinctive phrases (maybe matches 3 of 4)

11) See if filtering by date or character count can help us find some supporting comments

12) Create samples to read and memo on

-   samples from organized cluster -- to understand flavor & variations

14) Analyze government comments separately - put them in Zotero

15) Notes on things to be researched -- who's behind x phrase at Y time

16) Lit review - Panofsky on open science

17) Journal targets -- critique of transparency in big data journal, or STSy take on infrastructure \*\*

# Data cleaning problems

-   Currently don't have correct comment+pdf cell (think this is solved)
-   Currently don't have correct nonPdf count (think this is solved)
-   Some attachments have multiple potential download types (think this is solved)
-   Want to individually examine late comments which are typically specifically important
-   Some PDFs aren't parsed (silent failure)
-   Some files aren't pdfs (warned failure, usually hand-fixable)
-   Some files are restricted (warned failure, usually )
-   Some comments need to be combined (typically one comment with many attachments)

## PDF errors to deal with

-   PDF error: read ICCBased color space profile error
-   PDF error: not an ICC profile, invalid signature
-   PDF error: invalid font weight

## Goals for August:

-   Generate a list of non-PDFs or restricted files
-   Find a way of discovering non-parsed PDFs (and generate list of those)
-   Find R code for text-matching comments to each other or to mass-mail language
