---
title: "Scraping comments from Regulations.Gov"
author: "Harald Klimes, Bennett McIntosh"
date: "2022-07-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

demo = FALSE #Turn off to stop the demo calls beneath several of the functions below
```

# Background and Prerequisites

For a research project, we need to scrape comments submitted to the Environmental Protection Agency on a proposed rule. The comments are posted on [regulations.gov](https://www.regulations.gov/document/EPA-HQ-OA-2018-0259-9322/comment).

We would like this scraping to produce:

1)  A data frame containing the text of every comment on the docket, as well as assorted metadata about the comments and the docket such as submitter name/type and objectID

2)  A directory containing PDFs (or similar NVivo-legible document) of every comment, in subdirectories named by comment ID for future search/analysis. The documents in this directory should include both the attachments to the comments themselves, as well as a holistic document, with a table of contents, that combines the comment and all attachments

Documentation for the regulations.gov API is available [here](https://open.gsa.gov/api/regulationsgov/). An API key can be requested [here](https://api.data.gov/signup/).

The below code assumes you have saved your API key as "regulations_gov_key" in your `renviron`. We also make use of a few libraries:

```{r}
library(jsonlite)
library(httr)
library(tidyverse)
library(pdftools)
library(dplyr)
library(stringr)

api_key <- Sys.getenv("regulations_gov_key")

```

# Affordances and constraints

## Rate Limits

The GSA imposes a [rate limit](https://api.data.gov/docs/rate-limits/) of 1000 requests per hour (on a rolling basis) for the api keys it distributes. To avoid running afoul of these, we use a wrapper for the GET() function that simply pauses execution when we approach the limits. This is not the most efficient solution, since the rate limits function on a rolling basis (so if we made our first request at 10:15 and our 1000th at 10:25 we could start making new requests at 11:15, not 11:25), but it will serve for the scale at which we are working.

NOTE: Since at current speeds below code takes \~8 minutes to make 1k requests, one kludge would be sleeping for 55 minutes rather than 60, which would save \~5min per thousand comments (a little under 2 hours for the \~23k comments on the 2020 supplement)

```{r}
get_or_wait <- function(url) {
  
  # GET the item from the URL given
  r <- GET(url)
  
  # Your remaining requests before the rate limit are stored in r$headers
  remaining <- as.integer(r$headers$`x-ratelimit-remaining`)
  
  # If remaining requests are low, sleep for an hour and then resume execution, reporting the action in "log.txt"
  if (remaining < 5) {
    pauseTime <- lubridate::now()
    cat(paste0("\n APPROACHING RATE LIMIT: pausing until 55min from ", pauseTime, "\n",
               "requests remaining currently: ", remaining, "\n"),
        file = "log.txt", append = TRUE)
    Sys.sleep(3300)
    restartTime <- lubridate::now()
    r <- GET(url)
    remaining <- as.integer(r$headers$`x-ratelimit-remaining`)
    cat(paste0("Restarting at ", restartTime, "; requests remaining currently: ", remaining, "\n"), 
        file = "log.txt", append = TRUE)
  }
  return(r)
}
```

Note the messages saved to `log.txt`. These (and others below) are the ones I have found most useful in debugging; running this code, especially the download function below, generates many other notes and warnings to `stdout` that I have found less useful, so tend to pipe to pipe to a separate file such as `out.txt`. To facilitate this, both `log.txt` and `out.txt` are currently listed in `.gitignore`

## Page limits

In addition to the rate limits mentioned above, the GSA places a few other limits on the API:

1)  When pulling a page of comments, the page may not be larger than 250 comments

2)  When paging through several pages of comments, you may only access up to 20 pages (i.e. calls for page 21+ of a search are not valid)

This effectively means that no search can return more than 5,000 comments. For dockets with \~9k and \~22k comments, this is an issue! The GSA [currently recommends](https://open.gsa.gov/api/regulationsgov/#there-are-strict-pagination-limits-in-v4-how-do-i-retrieve-all-comments-in-a-docket-posted-on-the-same-day-if-the-number-of-comments-is-greater-than-2500) (in beta versions of v4 of the API) that you get around these issues by sorting your search results by `lastModifiedDate`, and then running several searches in series, each of which filters out items with a `lastModifiedDate` before the most recent result in your previous search. We will need to run 2 different searches for our 9k comments, and 5 for our 22k comments

The function below will assist us in paging through these searches. It gives us a page of comments, sorted by `lastModifiedDate`, from a search in which comments with a `lastModifiedDate` before `last_mod` filtered out if needed. Page size (`pSize`) defaults to the max, 250, but may be set as low as 5 (usually for testing purposes). Note that getting a page of comments on a given document requires that document's objectID, which we will find below.

```{r}
getAPageOfComments <- function(page, objectID, last_mod = "", pSize = 250){
  if (last_mod == "") {
    url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                  objectID, "&page[size]=", pSize, "&page[number]=", page,
                  "&sort=lastModifiedDate,documentId&api_key=", api_key)
  } else {
    url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                  objectID, "&filter[lastModifiedDate][ge]=", sub(" ", "%20", last_mod), 
                  "&page[size]=", pSize, "&page[number]=", page, 
                  "&sort=lastModifiedDate,documentId&api_key=", api_key)
  }
  # print(paste("getting a page of comments from", url))
  raw_comments <- get_or_wait(url)  
  comments <- fromJSON(rawToChar(raw_comments$content)) 
  return(comments)
}
```

# Targeting the right comments

## What are our objectIDs?

Every proposed rule, promulgated rule, comment, or supporting document has a unique `objectID`, a 16-character alphanumeric (since the only letters I've seen are a-f, presumably these are hexidecimal numbers somewhere in GSA's back-end). Within a given rule-making process or "docket," they also share a `docketID`, which is more human-readable. The `docketID`, suffixed by a hyphen and a few numerals, is also listed in each rule, comment, or supporting document as its `id`. So for instance, within the docket with `docketID` "EPA-HQ-OA-2018-0259", we may see a proposed rule with `id` "EPA-HQ-OA-2018-0259-0001", and a comment on said rule with `id` "EPA-HQ-OA-2018-0259-5041", each with a unique hexadecimal `objectID`

We use the `httr` package to retrieve summary information for the docket ID. Here, we're using the docket ID "EPA-HQ-OA-2018-0259" - your docket ID can be discovered by searching for your rule/proposal on [regulations.gov](regulations.gov). Additionally filtering for "Proposed Rule" during the `GET()` call allows us to pull all four documents of `documentType` "Proposed Rule" in one `GET()` call (rather than paging through several pages of documents, mostly of `documentType` "Supporting & Related Material")

```{r}

docketID <- "EPA-HQ-OA-2018-0259"

if (demo) { 
  df <- get_or_wait(url = paste0("https://api.regulations.gov/v4/documents?filter[docketId]=",
                         docketID, "&filter[documentType]=Proposed%20Rule&api_key=",
                         api_key)) 
}
```

With `jsonlite` we can parse the response and identify the object IDs for proposed rule:

```{r}
if (demo) { 
  df2 <- fromJSON(rawToChar(df$content))
  
  df2$data$attributes %>% 
    filter(documentType == "Proposed Rule")
}
```

We can see that the docket includes four documents of documentType "Proposed Rule": the original one from 2018, a modified one in 2020, and one deadline extension for each.

The deadline extensions do not appear to have any associated comments, and have subtype "Extension of Comment Period," which we can use to filter them out.

We can see from df2 that the `objectId` of the 2018 proposed rule is "090000648320bc9e", and for the 2020 supplement is "09000064846eebaf". These will allow retrieval of comments through the comments endpoint of the API

For now, we will focus on the 2018 proposed rule:

```{r}
oID <- "090000648320bc9e"
```

## How many comments?

We will also want to know how many comments we can expect. To do this, we request a page of comments on our proposed rule using its objectID. Note that the URL in this request only returns (in `$data`) the first page of comments meeting the requested criteria (pages default to 25 comments long), but since we are only interested in the `totalElements` field (in `meta`), this does not concern us for now.

```{r}
getCommentCount <- function(objectID){
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                objectID, "&api_key=", api_key)
  
  raw_comments <- get_or_wait(url)
  comments <- fromJSON(rawToChar(raw_comments$content))
  return(comments$meta$totalElements)
}

# get the comment count (will be useful below)
commentCount <- getCommentCount(oID)
if(demo){ commentCount }
```

## Exploring a single comment

When we get a page of comments with an API call like in `getAPageOfComments()` above, we only get limited information about each comment. To get more details, including the content of the comment and its attachments, we need to request the data at each of the urls in `$data$links$self` in turn. For a page of 25 comments, `$data$links$self` will be a page of 25 links, each of which we can pass to the `getSingleCommentDetails()` function below.

```{r}
getSingleCommentDetails <- function(link){
  raw_comment <- get_or_wait(paste0(link, "?include=attachments&api_key=", api_key))
  comment <- fromJSON(rawToChar(raw_comment$content))
}

# if demo = TRUE, you can explore the structure of a single comment if you like
if(demo){
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                oID, "&api_key=", api_key)
  raw_comments <- get_or_wait(url)
  comments <- fromJSON(rawToChar(raw_comments$content))
  single_comment <- getSingleCommentDetails(comments$data$links$self[1])
  single_comment
}
```

# Downloading attachments

Each comment's details provide link to the attachments for download. Often, there is just a single attachment: the text of the comment; the comment itself will either duplicate this attachment of say something like "see attached." Nevertheless, when we save and return the full text of a comment, we want both the comment body and the content of any attachments. This provides that. All attachments, as well as fullText.txt, are both saved in the directory `attachments/id` where ID is the more human readable ID (the docket ID suffixed with a hyphen and a few numerals). The function also returns the full text, which we will place in a data frame for future analysis.

Most of the body of this function is devoted to handling various rare edge cases:

1)  restricted (often copyrighted) attachments

2)  non-pdf files

3)  files are occasionally available in multiple formats, which may or may not include our chosen format

4)  we may also occasionally encounter items that are not attachments, but I have not encountered this.

```{r}
getTestComment <- function(id) {
  url <- paste0("https://api.regulations.gov/v4/comments/", id)
  return(getSingleCommentDetails(url))
}

downloadFilesAndGetText <- function(comment, destdir = "attachments"){

  # create the directory in which to download attachments
  commentDir <- comment$data$id
  if (!dir.exists(paste(destdir, commentDir, sep = "/"))) {
    dir.create(paste(destdir, commentDir, sep = "/"))
  }
  
  # add the text from the comment itself to the fullText string
  fullText <- comment$data$attributes$comment
  
  # if there are attachments, add them to the directory and to the fullText string
  if (!is.null(comment$included)) {

  
    # check the number of attachments
    N <- length(comment$included$id)
    
    
    # check which comments are restricted
    restricted <- !is.na(comment$included$attributes$restrictReasonType)
    
    for (i in 1:N){
      # should check if comment is restricted and skip download if so
      # currently breaks ONLY when there are multiple documents
      # simply changing to is.null doesn't work
      
      # tests if the file is restricted. 
      # Unfortunately this works differently for single-attachment and multi-attachment comments
      if (restricted[i]) {
        cat(paste("WARNING: attachment", i, "could not be downloaded from comment ",
                  comment$data$id, ", perhaps because of restrictReasonType: ", 
                  comment$included$attributes$restrictReasonType[i], "\n"),
            file="log.txt", append =TRUE)
        fullText <- paste(fullText, "\n\n---\n", "Attachment", i, "title:\n", comment$included$attributes$title[i], 
                          "\n---\n", "RESTRICTED (",comment$included$attributes$restrictReasonType[i], ")", sep = " ")
        next
      }
      
      itemFormat <- comment[["included"]][["attributes"]][["fileFormats"]][[i]][[2]]
      
      # if the item can be downloaded in several formats, set to PDF if possible
      if (length(itemFormat) > 1) {
        if ("pdf" %in% itemFormat) {
          fi <- match("pdf", itemFormat)
          itemFormat <- "pdf"
        } else {
          cat(paste("multiple formats, but none PDF; comment:", comment$data$id, "attachment: ", i, "\n"), 
              file="log.txt", append =TRUE)
          fi <- 1
        }
        itemFormat <- comment[["included"]][["attributes"]][["fileFormats"]][[i]][[2]][[fi]]
      } else {fi <- 1}
      
      
      
      # Warn the user if the file is not marked as an attachment or is not a pdf
      if (comment$included$type[i] != "attachments") {
        cat(paste0("WARNING, item ", i, "'s type in comment ", comment$data$id,
                   " is not 'attachments' but", comment$included$type, "\n"), 
            file="log.txt", append =TRUE)
      }
      
      # target the directory, filename, and extension
      filename <- paste0(comment$included$attributes$title[i], ".", itemFormat)
      destpath <- paste(destdir, commentDir, filename, sep = "/")
      
      # print(paste0("found ", filename, " an ", comment$included$type[i], " with format ",
      # itemFormat))
      
      # download the file to the targeted path
      download.file(comment$included$attributes$fileFormats[[i]]$fileUrl[fi], 
                    destfile = destpath, 
                    mode = "wb")
      
      # Warn the user if the file is not a PDF (and add placeholder to fullText)
      if (itemFormat != "pdf") {
        cat(paste0("WARNING, item ", i, "'s format in comment ", comment$data$id,
                   " is not 'pdf' but ", itemFormat, "\n"), file="log.txt", append =TRUE)
        fullText <- paste(fullText, "\n\n---\n", "Attachment", i, "title:\n", comment$included$attributes$title[i],
                          "\n---\n\n", "UNPARSED ", itemFormat, sep = " ")
      } # Otherwise, download the PDF and grab the text 
      else {
        attachmentText <- paste(pdf_text(destpath), collapse = "", sep = "/n")
        
        fullText <- paste(fullText, "\n\n---\n", "Attachment", i, "title:\n",
                          comment$included$attributes$title[i], "\n---\n\n", 
                          attachmentText, sep = " ")
        
      }
      
    }
    # TODO: combine all downloaded files into one big file - but idk if that's needed rn
    # TODO: simply write the new text to a file progressively (r/t storing as a string)
    #       (fullText can be a path if you want to do searching later)
    # TODO: fix so docs are downloaded/appended to .txt in order by the $docOrder list in $included$attributes
  }

  textFName <- paste(destdir, commentDir, "fullText.txt", sep = "/")
  cat(fullText, file = textFName, append = FALSE)
  
  return(fullText)
} 

testDownload <- function(id) {
  comment <- getTestComment(id)
  downloadFilesAndGetText(comment)
  return(comment)
} #destdir should be a directory - pulls filename from attachment data


if (demo) {
  c1037 <- testDownload("EPA-HQ-OA-2018-0259-1037") # multiple attachments, 1 restricted
  c0066 <- testDownload("EPA-HQ-OAR-2020-0532-0066") # multiple attachments, none restricted; #13 needs OCR
  c0445 <- testDownload("EPA-HQ-OA-2018-0259-0445") # single, restricted (copyrighted) attachment
  c0684 <- testDownload("EPA-HQ-OA-2018-0259-0684") # single, restricted (other) attachment
  c0671 <- testDownload("EPA-HQ-OA-2018-0259-0671") # attachment is docx, not pdf
  c1388 <- testDownload("EPA-HQ-OA-2018-0259-1388") # no attachment (comment body only)
}


```

The `if` statement above pulls some comments I have found useful for testing various edge cases

# Putting it together

## Helper functions

Before we finally run the request, a few helper functions help us log which comments have been downloaded and put all the comments together into a data frame.

```{r}
# nullToNA replaces NULL elements of a list with NA
nullToNA <- function(x) {
  x[sapply(x, is.null)] <- NA
  return(x)
}

# slug takes a function in the form of EPA-HQ-OA-####-####-####(#) and extracts everything after the final hyphen
slug <- function(id) {
  return(word(id, 6, sep = "-"))
}
```

## Page by page and batch by batch

The below uses nested loops in the form:

`while (not enough comments) {`

`for (each page in a batch) {`

`for (each comment on a page) {do stuff}}}}`

to add comments one by one to a data frame that carries their `attributes` and a list that carries their ID and text. Then, it zips the data frame and list together using cbind.

```{r, echo = FALSE}

# getCommentDf <- function(oID) {

  startTime <- lubridate::now()

  pageLen <- 250 # will be 250 once tested
  pagesPerBatch <- 20 # will be 5000/250 = 20 once tested
  commentCount <- getCommentCount(oID)
  last_mod <- ""
  batchN <- 1
  
  #setting comment count lower for testing
  # commentCount <- 900
  textList <- rep("ID TBD", commentCount)
  
  commentDF <- as.data.frame(list(no = "data"))
  commentsAdded <- 0
  
  cat(paste(startTime, ": Pulling", commentCount, "comments in batches of", pagesPerBatch, "pages of",
            pageLen, "comments each"), file = "log.txt", append = FALSE)
  
  stop = FALSE
  while (!stop) {
    
    for (pageN in 1:pagesPerBatch) {
      
      # don't bother adding more comments if we have all the comments we need
      # this will usually be the stop condition when collecting a full set of comments
      if(nrow(commentDF) == commentCount) { 
        stop = TRUE
        break 
      }
      
      # pull a page of comments
      cat(paste("\npulling page", pageN, "of batch", batchN, "\n"), 
          file="log.txt", append =TRUE)
      pageOfComments <- getAPageOfComments(pageN, oID, last_mod, pageLen)
      
      # for the first page on every batch but the first, find the comment where we left off
      if ((pageN == 1) & (textList[1] != "ID TBD")) { 
        start <- match(last_name, pageOfComments$data$id) + 1
        cat(paste0("starting batch ", batchN," at ", start, "th comment\n"), file="log.txt", append =TRUE)
      } else {
        start <- 1
      }
      
      
      # get every comment in the page, except the ones before the last comment we grabbed
      for (i in start:pageLen) {
        
        commentsAdded <- commentsAdded + 1
        
        # stop comment collection early if we've explicitly asked for fewer comments than exist
        if (commentsAdded > commentCount) {
          cat(paste0("WARNING: it looks like there's more comments than commentCount (",
                     commentCount, ")! Terminating..."), file = "log.txt", append = TRUE)
          stop = TRUE
          break
        }
        
        url <- paste0(pageOfComments$data$links$self[i])
        comment <- getSingleCommentDetails(url)
        
        # log that the comment has been pulled
        cat(paste0(commentsAdded, ":-", slug(comment$data$id), "..."), file="log.txt", append = TRUE)
        
        # put the comment's details into a data frame
        comment$data$attributes["displayProperties"] <- NA
        attsList <- nullToNA(comment$data$attributes)
        if (i == 1 & pageN == 1) {
          commentDF <- as.data.frame(attsList)
        } else { 
          commentDF <- rbind.data.frame(commentDF, as.data.frame(attsList))
        }

        names(textList)[commentsAdded] <- comment$data$id
        textList[commentsAdded] <- downloadFilesAndGetText(comment)
        
        #delete directory to save drive space during testing
        deldir <- paste("attachments", comment$data$id, sep = "/")
        unlink(deldir, recursive = TRUE)
      
      }
  
    }
    # The last_mod given is formatted yyyy-mm-ddThh:mm:ssZ (in UTC) but get() needs yyyy-mm-dd hh:mm:ss (in Eastern Time) 
    last_mod_UTC <- as.POSIXct(sub("T", " ", pageOfComments$data$attributes$lastModifiedDate[pageLen]), tz = "UTC")
    last_mod <- sub(" E*T", "", lubridate::with_tz(last_mod_UTC, "America/New_York"))
    last_name <- comment$data$id
    batchN <- batchN + 1
  }
  
  # zip the comments' attributes together with their names and text
  c <- cbind(names(textList), textList, commentDF)
  colnames(c)[1] <- "ID"
  colnames(c)[2] <- "full text"
  
  save(c, fname = "test9k.Rdata")
  
  # log the total time and time per comment
  endTime <- lubridate::now()
  runTime <- endTime - startTime
  timePerComment <- runTime / commentCount
  cat(paste("Run time of", runTime, "or", timePerComment, "per comment"))
  
```

# Next Steps

3)  Test rate limit handling and run through while deleting files to find edge cases
4)  OCR with PDFtools?
5)  Make unified PDF
6)  Statistical characterization:

<!-- -->

a)  How many non-unique comments
b)  Distribution of n_attachments
c)  Distribution of time submitted
d)  Any of these interact with institutional/individual/what was the other one categories?

<!-- -->

7)  Sort functions
8)  Search functions
