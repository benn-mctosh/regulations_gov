---
title: "Scraping comments from regulations.gov"
author: "Harald Kliems, Bennett McIntosh"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

For a research project, we need to scrape comments submitted to the Environmental Protection Agency on a proposed rule. The comments are posted on [regulations.gov](https://www.regulations.gov/document/EPA-HQ-OA-2018-0259-9322/comment).

Documentation for the regulations.gov is available [here](https://open.gsa.gov/api/regulationsgov/). An API key can be requested [here](https://api.data.gov/signup/). The below assumes you have saved your API key as "regulations_gov_key" in your `renviron`.

In this first code chunk, we also import several libraries.

```{r}
library(jsonlite)
library(httr)
library(tidyverse)
library(pdftools)

api_key <- Sys.getenv("regulations_gov_key")

```

We use the `httr` package to retrieve summary information for the docket ID. Here, we're using the docket ID "EPA-HQ-OA-2018-0259"

```{r}
df <- GET(url = paste0("https://api.regulations.gov/v4/documents?filter[docketId]=EPA-HQ-OA-2018-0259&api_key=", api_key))
```

With `jsonlite` we can parse the response and identify the object IDs for proposed rule:

```{r}

df2 <- fromJSON(rawToChar(df$content))

df2$data$attributes %>% 
  filter(documentType == "Proposed Rule")
```

We can see that the docket includes two proposed rules, the original one from 2018, and a supplemental Notice from 2020. Now the objectIDs allow retrieval of comments through the comments endpoint of the API

```{r}
df2$data$attributes %>% 
  filter(documentType == "Proposed Rule") %>% 
  pull()

comments <- GET(paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=090000648320bc9e&api_key=", api_key))


df <- GET(paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=090000648320bc9e&page[size]=250&page[number]=1&sort=lastModifiedDate,documentId&api_key=", api_key))

https://api.regulations.gov/v4/comments?filter[commentOnId]=09000064846eebaf&page[size]=250&page[number]=1&sort=lastModifiedDate,documentId&api_key=DEMO_KEY

comments <- fromJSON(rawToChar(comments$content))

view(comments$data)
```

We can only retrieve 250 comments at a time. So we build a function that takes a numeric argument for `page[number]` argument in the API call and returns the data component:

```{r}
retrieve_comment_ids <- function(page, last_mod = ""){
  if_else(last_mod == "",
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=090000648320bc9e&page[size]=250&page[number]=",
                page,
                "&sort=lastModifiedDate,documentId&api_key=",  api_key),
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=090000648320bc9e&",
                "filter[lastModifiedDate][ge]=",
                last_mod,
                "page[size]=250&page[number]=",
                page,
                "&sort=lastModifiedDate,documentId&api_key=", api_key)
  )
  comments <- GET(url)
  comments <- fromJSON(rawToChar(comments$content))

comments$data
}

retrieve_comment_ids <- function(page, last_mod = NA){
 
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=090000648320bc9e&page[size]=250&page[number]=",
                page,
                "&sort=lastModifiedDate,documentId&api_key=", api_key)
  
  comments <- GET(url)
  comments <- fromJSON(rawToChar(comments$content))

comments$data
}
```

Then we can use `map_df` to go through 250 comments at a time. The total number of comments is available in the `$meta$totalElements`. In addition to the limitation of only getting 250 comments at a time, the API also has a rate limit of 1000 requests per hour. Finally, there is a limit of retrieving only 5000 comments by paging through. All of this requires another function.

[https://api.regulations.gov/v4/comments?filter[commentOnId]=09000064846eebaf&filter[lastModifiedDate][ge]=2020-08-10](https://api.regulations.gov/v4/comments?filter%5BcommentOnId%5D=09000064846eebaf&filter%5BlastModifiedDate%5D%5Bge%5D=2020-08-10){.uri} 11:58:52&page[size]=250&page[number]=N&sort=lastModifiedDate,documentId&api_key=DEMO_KEY

```{r}
retrieve_all_comment_ids <- function(n){
  # retrieve number of comments
  df <- GET(paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=090000648320bc9e&api_key=", api_key))
  df2 <- fromJSON(rawToChar(df$content))
  n_comments <- df2$meta$totalElements
  n_pages <- as.integer(n_comments/250)
  
  if_else(n_comments <= 5000,
          retrieve_comment_ids(1:n_pages),
          
    
  )
}

pages <- comments

comment_ids <- map_df(1:37, retrieve_comment_ids)

```

Let's get a single comment:

```{r}
comments$data
GET(paste0(comments$data$links$self[1], "?api_key=", api_key))

df <- GET(paste0("https://api.regulations.gov/v4/comments/EPA-HQ-OA-2018-0259-6884?include=attachments&api_key=", api_key))

single_comment <-  fromJSON(rawToChar(df$content))

single_comment$included$attributes$fileFormats$

attachment <- GET(paste0(single_comment$included$links, "?api_key=", api_key))

comments <- fromJSON(rawToChar(attachment$content))
```

To get the link to an attachment file, you have to go deep into the JSON structure:

```{r}
download.file(single_comment$included$attributes$fileFormats[[1]]$fileUrl, destfile = "attachments/test.pdf", mode = "wb")
```

```{r}
pdf_text("attachments/test.pdf")
```

# Questions

-   Can there be more than one attachment?
-   BM: YES
-   Are all attachments OCR'd?
-   BM: Not necessarily. Even if all PDFs are OCR'd, there may be other visual media. From the FAQs: "You can attach up to 20 files, but each file cannot exceed 10MB. Valid file types include: .bmp, .docx, .gif, .jpeg, .jpg, .pdf, .png, .pptx, .rtf, .sgml, .tif, .tiff, .txt, .wpd, .xlsx, .xml." For now, it may be best to assume all attachments are OCR'd PDFs, get alerts when they're not (PDFs that look blank, and other filetypes?) and handle those in the future

# To-dos

-   Consider contacting data.gov helpdesk to increase rate limit to 2000 requests/hr. Other limits may be similarly flexible
-   Add exception handling to know how many non-OCR'd PDFs and non-PDF attachments we're getting
-   Briefly familiarize yourself with the following libraries:
-   pdftools
-   jsonlite
-   httr
-   tidyverse
-   Discuss how these files will be coded: should we concatenate all text?
