---
title: "Scraping comments from regulations.gov"
author: "Harald Kliems, Bennett McIntosh"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
# configure knitr
knitr::opts_chunk$set(echo = TRUE)
```

# Background

For a research project, we need to scrape comments submitted to the Environmental Protection Agency on a proposed rule. The comments are posted on [regulations.gov](https://www.regulations.gov/document/EPA-HQ-OA-2018-0259-9322/comment).

Documentation for the regulations.gov is available [here](https://open.gsa.gov/api/regulationsgov/). An API key can be requested [here](https://api.data.gov/signup/). The below code assumes you have saved your API key as "regulations_gov_key" in your `renviron`.

```{r}
#import libraries. Ignoring the conflicts identified with stats::filter(), stats::lag(), and jsonlite::flatten() for now
library(jsonlite)
library(httr)
library(tidyverse)
library(pdftools)

#get api_key from the R environment
api_key <- Sys.getenv("regulations_gov_key")

#setting a couple of project-specific IDs to avoid hardcoding
docketID <- "EPA-HQ-OA-2018-0259"

```

We use the `httr` package to retrieve summary information for the docket ID. Here, we're using the docket ID "EPA-HQ-OA-2018-0259" . Additionally filtering for "Proposed Rule" during the `GET()` call allows us to pull all four documents of `documentType` "Proposed Rule" in one `GET()` call (rather than paging through several pages of documents, mostly of `documentType` "Supporting & Related Material")

```{r}
df <- GET(url = paste0("https://api.regulations.gov/v4/documents?filter[docketId]=",
                       docketID,
                       "&filter[documentType]=Proposed%20Rule&api_key=",
                       api_key))
```

With `jsonlite` we can parse the response and identify the object IDs for proposed rule:

```{r}

df2 <- fromJSON(rawToChar(df$content))

df2$data$attributes %>% 
  filter(documentType == "Proposed Rule")
```

We can see that the docket includes four documents of documentType "Proposed Rule": the original one from 2018, a modified one in 2020, and one deadline extension for each.

The deadline extensions do not appear to have any associated comments, and have subtype "Extension of Comment Period," which we can use to filter them out. Now the objectIDs allow retrieval of comments through the comments endpoint of the API

We can see from df2 that the objectId of the 2018 proposed rule is "090000648320bc9e", and for the 2020 proposed rule is "09000064846eebaf". I need to learn more about jsonlite to set the object IDs automatically, for now we are manually entering the object IDs into the list named `objectIDs`.

```{r}

objectIDs <- list("090000648320bc9e","0900006484450a29")

# df2$data$attributes %>% 
#   filter(subtype != "Extension of Comment Period") %>% 
#   pull()

```

```{r}

comments <- GET(paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                       objectIDs[1], "&api_key=", api_key))


df <- GET(paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=0",
                 objectIDs[1],
                 "&page[size]=250&page[number]=1&sort=lastModifiedDate,documentId&api_key=",
                 api_key))

comments <- fromJSON(rawToChar(comments$content))

view(comments$data)
```

#Some Useful Functions

We can only retrieve 250 (so why are my `comments` dfs showing only 25 elements each?) comments at a time. So we build a function that takes a numeric argument for `page[number]` argument in the API call and returns the data component:

```{r}

# this function takes page number "page", an objectID, and a timestamp "last_mod" and returns the first 250 comments on object ID submitted at or after the timestamp

retrieve_comment_ids <- function(page, objectID, last_mod = ""){
  
  # if last_mod is blank, return the first 250 (chronologically) comment IDs
  if_else(last_mod == "",
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                objectID, "&page[size]=250&page[number]=",
                page,
                "&sort=lastModifiedDate,documentId&api_key=",
                api_key),
  
  # otherwise, return the first 250 on or after the timestamp
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=", objectID, "&",
                "filter[lastModifiedDate][ge]=",
                last_mod,
                "page[size]=250&page[number]=",
                page,
                "&sort=lastModifiedDate,documentId&api_key=", api_key)
  )
  comments <- GET(url)
  comments <- fromJSON(rawToChar(comments$content))

comments$data
}

# this does the same as the above, but without bothering with last_mod

retrieve_comment_ids <- function(page, objectID, last_mod = NA){
 
  url <- paste0("https://api.regulations.gov/v4/comments?filter[commentOnId]=",
                objectID, "&page[size]=250&page[number]=",
                page,
                "&sort=lastModifiedDate,documentId&api_key=", api_key)
  
  comments <- GET(url)
  comments <- fromJSON(rawToChar(comments$content))

comments$data
}
```

Then we can use `map_df` to go through 250 (25?) comments at a time. The total number of comments is available in the `$meta$totalElements`. In addition to the limitation of only getting 250 comments at a time, the API also has a rate limit of 1000 requests per hour. Finally, there is a limit of retrieving only 5000 comments by paging through. All of this requires another function.

[https://api.regulations.gov/v4/comments?filter[commentOnId]=09000064846eebaf&filter[lastModifiedDate][ge]=2020-08-10](https://api.regulations.gov/v4/comments?filter%5BcommentOnId%5D=09000064846eebaf&filter%5BlastModifiedDate%5D%5Bge%5D=2020-08-10){.uri} 11:58:52&page[size]=250&page[number]=N&sort=lastModifiedDate,documentId&api_key=DEMO_KEY

```{r}
retrieve_all_comment_ids <- function(n){
  # retrieve number of comments
  df <- GET(paste0("https://api.regulations.gov/v4/comments",
                   "?filter[commentOnId]=090000648320bc9e&api_key=", api_key))
  df2 <- fromJSON(rawToChar(df$content))
  n_comments <- df2$meta$totalElements
  n_pages <- as.integer(n_comments/250)
  
  # retrieve comments until we have 5000
  if_else(n_comments <= 5000,
          retrieve_comment_ids(1:n_pages, objectIDs[1]),
          
    
  )
}

pages <- comments

argList <- list(x = 1:37, y = objectIDs[1])
crossArg <- cross_df(argList)

# comment_ids <- map2_df(crossArg$x, crossArg$y,
#                        retrieve_comment_ids)

```

#Exploring a single comment

Let's get a single comment:

```{r}
comments$data
GET(paste0(comments$data$links$self[1], "?api_key=", api_key))

df <- GET(paste0("https://api.regulations.gov/v4/comments/EPA-HQ-OA-2018-0259-6884?include=attachments&api_key=", api_key))

single_comment <- fromJSON(rawToChar(df$content))

# single_comment$included$attributes$fileFormats$attachment <-- NA
# 
# attachment <- GET(paste0(single_comment$included$links, "?api_key=", api_key))
# 
# comments <- fromJSON(rawToChar(attachment$content))
```

To get the link to an attachment file, you have to go deep into the JSON structure:

```{r}
download.file(single_comment$included$attributes$fileFormats[[1]]$fileUrl, destfile = "attachments/test.pdf", mode = "wb")
```

```{r}
text <- pdf_text("attachments/test.pdf")
```

# Questions on design

-   Can there be more than one attachment?
-   BM: YES, seems to be rare though. Example comment [here](https://www.regulations.gov/comment/EPA-HQ-OAR-2020-0532-0066) (note this is not on the Transparency rule we are concerned with, but could be useful for troubleshooting)

-   Are all attachments OCR'd?
-   BM: Not necessarily. Even if all PDFs are OCR'd, there may be other visual media. From the FAQs: "You can attach up to 20 files, but each file cannot exceed 10MB. Valid file types include: .bmp, .docx, .gif, .jpeg, .jpg, .pdf, .png, .pptx, .rtf, .sgml, .tif, .tiff, .txt, .wpd, .xlsx, .xml." For now, it may be best to assume all attachments are OCR'd PDFs, get alerts when they're not (PDFs lacking OCR, and other filetypes?) and handle those in the future

-   Do we need the `last_mod` input to these functions?

# Questions on specs
-   Discuss how these files will be coded: should we concatenate all text in a given comment?
-   Will we want to de-duplicate mass-mail campaigns (looks like agencies already do this somewhat)?
-   Assume we'll want to retain which proposal comment is on. What other metadata matter (there's lots, but thinking about filtering it to a manageable level)
    - Date
    - Submitter
    - Somehow determine if supporting or opposing proposed rule? That's a stretch goal for my PAship, but we can talk about how to do that
 

# Minimum viable product to-dos

-   Consider contacting data.gov helpdesk to increase rate limit to 2000 requests/hr. Other limits may be similarly flexible
-   Add exception handling to know how many non-OCR'd PDFs, non-PDF attachments, and multiple attachments we're getting
-   Briefly familiarize yourself with the following libraries:
-   pdftools
-   jsonlite
-   httr
-   tidyverse

# Modularity & quality of life to-dos

-   figure out why we're pulling 25 comments at a time rather than 250 (is it just a page size limit?)
-   add system for changing docket ID
-   add system to recognize attachments other than OCR'd PDFs and handle them separately
-   think about de-duplicating data (it [looks like](https://www.regulations.gov/faq?anchor=find) the agencies may already do this to some extent - the posted comments are a small subset of the received comments, including filtering out near-duplicates from mass-mail campaigns)
-   When we search/code these, will we be doing fuzzy matcing? 
